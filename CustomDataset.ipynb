{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03f3a58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob as glob\n",
    "\n",
    "from xml.etree import ElementTree as et\n",
    "from config import (\n",
    "    CLASSES, RESIZE_TO, \n",
    "    TRAIN_DIR_IMAGES, VALID_DIR_IMAGES, \n",
    "    TRAIN_DIR_LABELS, VALID_DIR_LABELS,\n",
    "    BATCH_SIZE\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from custom_utils import collate_fn, get_train_transform, get_valid_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cb267fd-480a-4d7d-bd02-e4d2f0dff01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9af682c3-3118-4cee-b722-2463154783f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bboxes': [[[45.87416548610522, 321.0748212962833], [80.88455201672716, 418.18666836501575]], [[302.2567837910538, 306.2475239357931], [307.9105510864276, 331.9976051656832]], [[427.6693363071177, 306.5220569242455], [435.1162173731115, 333.624284056307]]], 'pedestrian_class': [9, 9, 9]}\n"
     ]
    }
   ],
   "source": [
    "def read_dicts_from_file(file_path):\n",
    "    lines = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    dictionary = ''\n",
    "    for line in lines:\n",
    "        line = line.replace('\\n','').split(' ')\n",
    "        line = [s for s in line if len(s) != 0]\n",
    "        for s in line:\n",
    "            dictionary = dictionary + s\n",
    "            \n",
    "    return ast.literal_eval(dictionary)\n",
    "\n",
    "# Example usage\n",
    "file_path = './input/train_txts/000032.txt'\n",
    "dictionaries = read_dicts_from_file(file_path)\n",
    "\n",
    "# Now 'dictionaries' is a list containing dictionaries from the file\n",
    "print(dictionaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60601eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, images_path, labels_path, \n",
    "        width, height, classes, transforms=None\n",
    "    ):\n",
    "        self.transforms = transforms\n",
    "        self.images_path = images_path\n",
    "        self.labels_path = labels_path\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.classes = classes\n",
    "        self.image_file_types = ['*.jpg', '*.jpeg', '*.png', '*.ppm']\n",
    "        self.all_image_paths = []\n",
    "        self.dictionary = dict()\n",
    "        \n",
    "        # get all the image paths in sorted order\n",
    "        for file_type in self.image_file_types:\n",
    "            self.all_image_paths.extend(glob.glob(f\"{self.images_path}/{file_type}\"))\n",
    "        self.all_annot_paths = glob.glob(f\"{self.labels_path}/*.txt\")\n",
    "        \n",
    "        # Remove all annotations and images when no object is present.\n",
    "        self.read_and_clean()\n",
    "        self.all_images = [image_path.split(os.path.sep)[-1] for image_path in self.all_image_paths]\n",
    "        self.all_images = sorted(self.all_images)\n",
    "    \n",
    "    def read_and_clean(self):\n",
    "        \"\"\"\n",
    "        This function will discard any images and labels when the XML \n",
    "        file does not contain any object.\n",
    "        \"\"\"\n",
    "        for annot_path in self.all_annot_paths:\n",
    "\n",
    "            self.dictionary = read_dicts_from_file(annot_path)\n",
    "            bbox = self.dictionary['bboxes']\n",
    "            \n",
    "            if len(bbox) == 0:\n",
    "                print(f\"Removing {annot_path} and corresponding image\")\n",
    "                self.all_annot_paths.remove(annot_path)\n",
    "                self.all_image_paths.remove(annot_path.split('.txt')[0]+'.jpg')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # capture the image name and the full image path\n",
    "        image_name = self.all_images[idx]\n",
    "        image_path = os.path.join(self.images_path, image_name)\n",
    "\n",
    "        # read the image\n",
    "        image = cv2.imread(image_path)\n",
    "        # convert BGR to RGB color format\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image_resized = cv2.resize(image, (self.width, self.height))\n",
    "        image_resized /= 255.0\n",
    "        \n",
    "        # capture the corresponding XML file for getting the annotations\n",
    "        annot_filename = image_name[:-4] + '.txt'\n",
    "        annot_file_path = os.path.join(self.labels_path, annot_filename)\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        # tree = et.parse(annot_file_path)\n",
    "        # root = tree.getroot()\n",
    "        \n",
    "        # get the height and width of the image\n",
    "        image_width = image.shape[1]\n",
    "        image_height = image.shape[0]\n",
    "\n",
    "        dictionary = read_dicts_from_file(annot_file_path)\n",
    "        bboxes = dictionary['bboxes']\n",
    "        labels = dictionary['pedestrian_class']\n",
    "        \n",
    "        # box coordinates for xml files are extracted and corrected for image size given\n",
    "        # for member in root.findall('object'):\n",
    "        for bbox, label in zip(bboxes, labels):\n",
    "            # map the current object name to `classes` list to get...\n",
    "            # ... the label index and append to `labels` list\n",
    "            bbox_ = bbox\n",
    "            labels.append(label)\n",
    "          \n",
    "            \n",
    "            # xmin = left corner x-coordinates\n",
    "            xmin = bbox_[0][0]\n",
    "            # xmax = right corner x-coordinates\n",
    "            xmax = bbox_[1][0]\n",
    "            # ymin = left corner y-coordinates\n",
    "            ymin = bbox_[0][1]\n",
    "            # ymax = right corner y-coordinates\n",
    "            ymax = bbox_[1][1]\n",
    "            \n",
    "            # resize the bounding boxes according to the...\n",
    "            # ... desired `width`, `height`\n",
    "            xmin_final = (xmin/image_width)*self.width\n",
    "            xmax_final = (xmax/image_width)*self.width\n",
    "            ymin_final = (ymin/image_height)*self.height\n",
    "            ymax_final = (ymax/image_height)*self.height\n",
    "            \n",
    "            boxes.append([xmin_final, ymin_final, xmax_final, ymax_final])\n",
    "\n",
    "        # bounding box to tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # area of the bounding boxes\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # no crowd instances\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "        # labels to tensor\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        # prepare the final `target` dictionary\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        image_id = torch.tensor([idx])\n",
    "        target[\"image_id\"] = image_id\n",
    "        # apply the image transforms\n",
    "        if self.transforms:\n",
    "            sample = self.transforms(image=image_resized,\n",
    "                                     bboxes=target['boxes'],\n",
    "                                     labels=labels)\n",
    "            image_resized = sample['image']\n",
    "            target['boxes'] = torch.Tensor(sample['bboxes'])\n",
    "            \n",
    "        return image_resized, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_images)\n",
    "\n",
    "# prepare the final datasets and data loaders\n",
    "def create_train_dataset():\n",
    "    train_dataset = CustomDataset(\n",
    "        TRAIN_DIR_IMAGES, TRAIN_DIR_LABELS,\n",
    "        RESIZE_TO, RESIZE_TO, CLASSES, get_train_transform()\n",
    "    )\n",
    "    return train_dataset\n",
    "def create_valid_dataset():\n",
    "    valid_dataset = CustomDataset(\n",
    "        VALID_DIR_IMAGES, VALID_DIR_LABELS, \n",
    "        RESIZE_TO, RESIZE_TO, CLASSES, get_valid_transform()\n",
    "    )\n",
    "    return valid_dataset\n",
    "\n",
    "def create_train_loader(train_dataset, num_workers=0):\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    return train_loader\n",
    "def create_valid_loader(valid_dataset, num_workers=0):\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    return valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb3f116b-2b23-480e-8a08-e6cfd7fbcf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 75\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomDataset(\n",
    "    TRAIN_DIR_IMAGES, TRAIN_DIR_LABELS, RESIZE_TO, RESIZE_TO, CLASSES\n",
    ")\n",
    "print(f\"Number of training images: {len(dataset)}\")\n",
    "\n",
    "# function to visualize a single sample\n",
    "def visualize_sample(image, target):\n",
    "    for box_num in range(len(target['boxes'])):\n",
    "        box = target['boxes'][box_num]\n",
    "        label = 'Pedestrian'#CLASSES[target['labels'][box_num]]\n",
    "        cv2.rectangle(\n",
    "            image, \n",
    "            (int(box[0]), int(box[1])), (int(box[2]), int(box[3])),\n",
    "            (0, 255, 0), 2\n",
    "        )\n",
    "        cv2.putText(\n",
    "            image, label, (int(box[0]), int(box[1]-5)), \n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2\n",
    "        )\n",
    "    cv2.imshow('Image', image)\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "NUM_SAMPLES_TO_VISUALIZE = 20\n",
    "for i in range(NUM_SAMPLES_TO_VISUALIZE):\n",
    "    image, target = dataset[i]\n",
    "    visualize_sample(image, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89ff4bd-ed3c-4c3c-a736-7566dff2b55a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
